{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helpers_py\n",
    "from nltk import word_tokenize\n",
    "import pickle #to load Glove\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding,LSTM, Bidirectional, Conv1D, GlobalMaxPooling1D, Dense, Dropout  \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "05ca393a-db89-4057-bb98-9ea4b22ebe11",
    "_uuid": "4caf9169-5f89-426a-b65d-0ec91ae7dc18"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "test_data = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n",
    "test_target = pd.read_csv(\"../input/test-data-with-the-target/test_target.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b6176aa8-40c3-4ed2-9414-463d5334b8d5",
    "_uuid": "624230b6-e1e4-4f0c-acf5-e8f22d672d16"
   },
   "outputs": [],
   "source": [
    "train_data['text_clean'] = train_data['text'].apply(lambda x : helpers_py.data_preprocessing(x))\n",
    "test_data['text_clean'] = test_data['text'].apply(lambda x : helpers_py.data_preprocessing(x))\n",
    "train_data = helpers_py.fix_labels(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Tokenizing with NTLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [word_tokenize(sen) for sen in train_data.text_clean]\n",
    "test_tokens = [word_tokenize(sen) for sen in test_data.text_clean]\n",
    "train_data['tokens'] = train_tokens\n",
    "test_data['tokens'] = test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncoder(data):\n",
    "    Real, Not_Real = [],[]\n",
    "    for target in data.target_fixed:\n",
    "        if target == 1:\n",
    "            Real.append(1)\n",
    "            Not_Real.append(0)\n",
    "        elif target == 0:\n",
    "            Real.append(0)\n",
    "            Not_Real.append(1)\n",
    "    data['Real'] = Real\n",
    "    data['Not_Real']= Not_Real\n",
    "    data = data[['id','keyword','location','text','text_clean','tokens', 'target', 'target_fixed','Real', 'Not_Real']]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = OneHotEncoder(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the train vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = [word for tokens in train_data[\"tokens\"] for word in tokens]\n",
    "train_vocab = sorted(list(set(train_words)))\n",
    "print(\"The total number of words in the train vocabulary:\", len(train_words))\n",
    "print(\"The total number of words in the train vocabulary:\",  len(train_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](http://)Building the test vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [word for tokens in test_data[\"tokens\"] for word in tokens]\n",
    "test_vocab = sorted(list(set(test_words)))\n",
    "print(\"The total number of words in the test vocabulary:\", len(test_words))\n",
    "print(\"The total number of words in the test vocabulary:\",  len(test_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Glove pretrained vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', 'rb') as fp:\n",
    "    glove = pickle.load(fp)\n",
    "print ('Glove is Loaded ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the train dictionary with Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = len(train_vocab), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(train_data[\"text_clean\"].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(train_data[\"text_clean\"].tolist())\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data[\"text_clean\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding sequences to the maximum sequece length from EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad_seq = pad_sequences(training_sequences, maxlen=23)\n",
    "test_pad_seq = pad_sequences(test_sequences, maxlen=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Glove embedding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index) + 1, 300))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = glove[word] if word in glove else np.random.rand(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiLSTM_CNN_Model(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    embedded_sequences = Dropout(0.3)(embedded_sequences)\n",
    "    l_lstm = Bidirectional(LSTM(300, activation = 'relu', return_sequences = True))(embedded_sequences)\n",
    "    l_conv1 = Conv1D(filters=300, kernel_size=2, activation='relu')(l_lstm)\n",
    "    l_conv2 = Conv1D(filters=300, kernel_size=3, activation='relu')(l_conv1)\n",
    "    l_conv3 = Conv1D(filters=300, kernel_size=4, activation='relu')(l_conv2)\n",
    "    l_pool = GlobalMaxPooling1D()(l_conv3)\n",
    "    l_d1 = Dense(150, activation='relu')(l_pool)\n",
    "    l_d1 = Dropout(0.2)(l_d1)\n",
    "    l_d2 = Dense(75, activation='relu')(l_d1)\n",
    "    l_d2 = Dropout(0.2)(l_d2)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(l_d2)\n",
    "    \n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_pad_seq\n",
    "label_names = ['Real', 'Not_Real']\n",
    "y_train = train_data[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CNN_Model(train_embedding_weights, 23, len(train_word_index)+1, 300, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.2, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_pad_seq, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_target['target']\n",
    "y_pred = prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score: \", accuracy_score(y_true, y_pred))\n",
    "print(\"Precision score: \", precision_score(y_true, y_pred))\n",
    "print(\"Recall score: \", recall_score(y_true, y_pred))\n",
    "print(\"F1 score: \", f1_score(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
